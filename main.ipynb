{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.optim import lr_scheduler\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import math\n",
    "import numpy\n",
    "d2l.use_svg_display()\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######定义VGG块\n",
    "def vgg_block(num_convs, in_channels, out_channels):\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels,\n",
    "                                kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "    return nn.Sequential(*layers)\n",
    "######定义NIN块\n",
    "def nin_block(in_channels, out_channels, kernel_size, strides, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU())\n",
    "######定义Inception块\n",
    "class Inception(nn.Module):\n",
    "    # c1--c4是每条路径的输出通道数\n",
    "    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception, self).__init__(**kwargs)\n",
    "        # 线路1，单1x1卷积层\n",
    "        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n",
    "        # 线路2，1x1卷积层后接3x3卷积层\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路3，1x1卷积层后接5x5卷积层\n",
    "        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\n",
    "        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n",
    "        # 线路4，3x3最大汇聚层后接1x1卷积层\n",
    "        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = F.relu(self.p1_1(x))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        p4 = F.relu(self.p4_2(self.p4_1(x)))\n",
    "        # 在通道维度上连结输出\n",
    "        return torch.cat((p1, p2, p3, p4), dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):  #@save\n",
    "    def __init__(self, input_channels, num_channels,\n",
    "                 use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_channels,\n",
    "                               kernel_size=3, padding=1, stride=strides)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels,\n",
    "                               kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2d(input_channels, num_channels,\n",
    "                                   kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        Y += X\n",
    "        return F.relu(Y)\n",
    "def resnet_block(input_channels, num_channels, num_residuals,\n",
    "                 first_block=False):\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            blk.append(Residual(input_channels, num_channels,\n",
    "                                use_1x1conv=True, strides=2))\n",
    "        else:\n",
    "            blk.append(Residual(num_channels, num_channels))\n",
    "    return blk\n",
    "# b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "#                    nn.BatchNorm2d(64), nn.ReLU(),\n",
    "#                    nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "# b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))\n",
    "# b3 = nn.Sequential(*resnet_block(64, 128, 2))\n",
    "# b5 = nn.Sequential(*resnet_block(128, 512, 2))\n",
    "# little_resnet = nn.Sequential(b1, b2, b3, b5,\n",
    "#                     nn.AdaptiveAvgPool2d((1,1)),\n",
    "#                     nn.Flatten(), nn.Linear(512, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(), \n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),\n",
    "    nn.Linear(120, 84), nn.Sigmoid(),\n",
    "    nn.Linear(84, 10))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######使用ReLu\n",
    "# net = nn.Sequential(\n",
    "#     nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.ReLU(),\n",
    "#     nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "#     nn.Conv2d(6, 16, kernel_size=5), nn.ReLU(),\n",
    "#     nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "#     nn.Flatten(),\n",
    "#     nn.Linear(16 * 5 * 5, 120), nn.ReLU(),\n",
    "#     nn.Linear(120, 84), nn.ReLU(),\n",
    "#     nn.Linear(84, 10))\n",
    "\n",
    "#######添加批量标准化\n",
    "# net = nn.Sequential(\n",
    "#     nn.Conv2d(1, 6, kernel_size=5, padding=2), \n",
    "#     nn.BatchNorm2d(6),  \n",
    "#     nn.Sigmoid(), \n",
    "#     nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "#     nn.Conv2d(6, 16, kernel_size=5), \n",
    "#     nn.BatchNorm2d(16),  \n",
    "#     nn.Sigmoid(),\n",
    "#     nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "#     nn.Flatten(),\n",
    "#     nn.Linear(16 * 5 * 5, 120), \n",
    "#     nn.BatchNorm1d(120), \n",
    "#     nn.Sigmoid(),\n",
    "#     nn.Linear(120, 84), \n",
    "#     nn.BatchNorm1d(84),  \n",
    "#     nn.Sigmoid(),\n",
    "#     nn.Linear(84, 10)\n",
    "#     )\n",
    "\n",
    "# ##########调整卷积大小\n",
    "# net = nn.Sequential(\n",
    "#     nn.Conv2d(1, 6, kernel_size=3, padding=1), nn.Sigmoid(), \n",
    "#     nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "#     nn.Conv2d(6, 16, kernel_size=3,padding=2,stride=3), nn.Sigmoid(),\n",
    "#     nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "#     nn.Flatten(),\n",
    "#     nn.Linear(16 * 3 * 3, 64), nn.Sigmoid(),\n",
    "#     nn.Linear(64, 32), nn.Sigmoid(),\n",
    "#     nn.Linear(32, 10))\n",
    "\n",
    "###########改变输出通道数量\n",
    "# net = nn.Sequential(\n",
    "#     nn.Conv2d(1, 16, kernel_size=5, padding=2), nn.Sigmoid(), \n",
    "#     nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "#     nn.Conv2d(16, 32, kernel_size=5), nn.Sigmoid(),\n",
    "#     nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "#     nn.Flatten(),\n",
    "#     nn.Linear(32 * 5 * 5, 240), nn.Sigmoid(),\n",
    "#     nn.Linear(240, 64), nn.Sigmoid(),\n",
    "#     nn.Linear(64, 10))\n",
    "\n",
    "########改变卷积层数——减少卷积层\n",
    "# net = nn.Sequential(\n",
    "#     nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(), \n",
    "#     nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "#     nn.Flatten(),\n",
    "#     nn.Linear(6 * 14 * 14, 120), nn.Sigmoid(),\n",
    "#     nn.Linear(120, 84), nn.Sigmoid(),\n",
    "#     nn.Linear(84, 10))\n",
    "########改变卷积层数——增加卷积层\n",
    "# net = nn.Sequential(\n",
    "#     nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(), \n",
    "#     nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "#     nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),\n",
    "#     nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "#     nn.Conv2d(16,16,kernel_size=3,padding=1),nn.Sigmoid(),\n",
    "#     nn.AvgPool2d(kernel_size=2,stride=1),\n",
    "#     nn.Flatten(),\n",
    "#     nn.Linear(16 * 4 * 4, 120), nn.Sigmoid(),\n",
    "#     nn.Linear(120, 84), nn.Sigmoid(),\n",
    "#     nn.Linear(84, 10))\n",
    "\n",
    "############使用MaxPooling\n",
    "# net = nn.Sequential(\n",
    "#     nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(), \n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#     nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#     nn.Flatten(),\n",
    "#     nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),\n",
    "#     nn.Linear(120, 84), nn.Sigmoid(),\n",
    "#     nn.Linear(84, 10))\n",
    "\n",
    "###########使用vgg块\n",
    "# net = nn.Sequential(\n",
    "#     vgg_block(num_convs=2,in_channels=1,out_channels=6), nn.Sigmoid(), \n",
    "#     nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "#     nn.Conv2d(6, 16, kernel_size=5,padding=2,stride=2), nn.Sigmoid(),\n",
    "#     nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "#     nn.Flatten(),\n",
    "#     nn.Linear(16 * 2 * 2, 32), nn.Sigmoid(),\n",
    "#     nn.Linear(32, 16), nn.Sigmoid(),\n",
    "#     nn.Linear(16, 10))\n",
    "\n",
    "##########使用NIN块\n",
    "# net = nn.Sequential(\n",
    "#     nn.Conv2d(1, 6, kernel_size=5,stride=1, padding=2), nn.Sigmoid(), \n",
    "#     nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "#     nin_block(6, 16, kernel_size=5,strides=1,padding=0), nn.Sigmoid(),\n",
    "#     nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "#     nn.Flatten(),\n",
    "#     nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),\n",
    "#     nn.Linear(120, 84), nn.Sigmoid(),\n",
    "#     nn.Linear(84, 10))\n",
    "\n",
    "##########使用Inception块\n",
    "# net = nn.Sequential(\n",
    "#     nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(), \n",
    "#     nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "#     Inception(6,6,(12,6),(10,2),2), nn.Sigmoid(),\n",
    "#     nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "#     nn.Flatten(),\n",
    "#     nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),\n",
    "#     nn.Linear(120, 84), nn.Sigmoid(),\n",
    "#     nn.Linear(84, 10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy_gpu(net, data_iter, device=None): #@save\n",
    "    \"\"\"使用GPU计算模型在数据集上的精度\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        net.eval()  # 设置为评估模式\n",
    "        if not device:\n",
    "            device = next(iter(net.parameters())).device\n",
    "    # 正确预测的数量，总预测的数量\n",
    "    metric = d2l.Accumulator(2)\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(X, list):\n",
    "                # BERT微调所需的（之后将介绍）\n",
    "                X = [x.to(device) for x in X]\n",
    "            else:\n",
    "                X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            metric.add(d2l.accuracy(net(X), y), y.numel())\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):\n",
    "    \"\"\"用GPU训练模型(在第六章定义)\"\"\"\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "    print('training on', device)\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    ####学习率指数衰减\n",
    "    #scheduler = lr_scheduler.ExponentialLR(optimizer,0.8)\n",
    "    ####学习率分段衰减\n",
    "    #scheduler = lr_scheduler.StepLR(optimizer=optimizer,step_size=10,gamma=0.6,verbose=True)\n",
    "    ####学习率余弦衰减\n",
    "    #scheduler = lr_scheduler.CosineAnnealingLR(optimizer=optimizer,T_max=10,eta_min=0.01)\n",
    "\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
    "                            legend=['train loss', 'train acc', 'test acc'])\n",
    "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练损失之和，训练准确率之和，样本数\n",
    "        metric = d2l.Accumulator(3)\n",
    "        net.train()\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            optimizer.zero_grad()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "            timer.stop()\n",
    "            train_l = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                             (train_l, train_acc, None))\n",
    "        test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
    "        animator.add(epoch + 1, (None, None, test_acc))\n",
    "        \n",
    "        ####使用学习率调度器\n",
    "        #scheduler.step()\n",
    "        \n",
    "    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '\n",
    "          f'test acc {test_acc:.3f}')\n",
    "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\n",
    "          f'on {str(device)}')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs = 0.9, 50 \n",
    "train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########查看网络中间层输出\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def see_picture(x):\n",
    "    if len(x.shape) == 4:\n",
    "        # 多张图像，每张图像单独显示\n",
    "        num_images = x.shape[1]\n",
    "        for i in range(num_images):\n",
    "            img = x[:, i].detach().squeeze().numpy()  # 去除多余的维度，并转换为NumPy数组\n",
    "            plt.imshow(img)  # 使用灰度色彩映射\n",
    "            plt.axis('off')  # 不显示坐标轴\n",
    "            plt.show()\n",
    "    elif len(x.shape) == 3:\n",
    "        # 单张图像，直接显示\n",
    "        img = x.detach().squeeze().numpy()  # 去除多余的维度，并转换为NumPy数组\n",
    "        plt.imshow(img)  # 使用灰度色彩映射\n",
    "        plt.axis('off')  # 不显示坐标轴\n",
    "        plt.show()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid input shape. Expected 3 or 4 dimensions.\")\n",
    "net.to('cpu')\n",
    "net.eval()\n",
    "x =train_iter.dataset[0][0]\n",
    "x = x.unsqueeze(0)  # 添加一个维度作为批量大小\n",
    "print(x.shape) \n",
    "see_picture(x)\n",
    "conv_1 = net[0]\n",
    "sigmoid_1 = net[1]\n",
    "avgpool_1 = net[2]\n",
    "conv_2 = net[3]\n",
    "sigmoid_2 = net[4]\n",
    "avgpool_2 = net[5]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_1_out = conv_1(x)\n",
    "print('经过第一个卷积层的输出为：')\n",
    "see_picture(conv_1_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_1_out = sigmoid_1(conv_1_out)\n",
    "print('经过第一个激活层的输出为：')\n",
    "see_picture(sigmoid_1_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgpool_1_out = avgpool_1(sigmoid_1_out)\n",
    "print('经过第一个池化层的输出为：')\n",
    "see_picture(avgpool_1_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_2_out = conv_2(avgpool_1_out)\n",
    "print('经过第二个卷积层的输出为：')\n",
    "see_picture(conv_2_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_2_out = sigmoid_2(conv_2_out)\n",
    "print('经过第二个激活层的输出为：')\n",
    "see_picture(sigmoid_2_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgpool_2_out = avgpool_2(sigmoid_2_out)\n",
    "print('经过第二个池化层的输出为：')\n",
    "see_picture(avgpool_2_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
